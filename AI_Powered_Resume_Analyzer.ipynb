{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PyPDF2._reader.PdfReader object at 0x00000201FD81FB10>\n"
     ]
    }
   ],
   "source": [
    "pdf = \"C:\\\\Users\\\\HarishVReddy\\\\OneDrive\\\\Desktop\\\\Harish\\\\VS Code\\\\Python\\\\NLP\\\\projects\\\\Harish_resume (3).pdf\"\n",
    "pdf_reader = PdfReader(pdf)\n",
    "print(pdf_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V HARISH\n",
      "♂phone9047214841 /envel⌢peharishvreddy10@gmail.com /linkedinLinkedIn /githubGithub\n",
      "Career Objective\n",
      "Motivated Tech-Enthusiast skilled in programming with a passion for problem-solving. Quick learner with an\n",
      "ever-growing hunger for new technologies. Seeking a challenging position in a reputable organization to expand\n",
      "and utilize my learning, skills, and knowledge.\n",
      "Academic Details\n",
      "Amrita Vishwa Vidyapeetham Oct 2022 – Present\n",
      "Bachelor of Technology in Computer Science and Engineering, Artiﬁcial Intelligence – CGPA: 8.41 Bengaluru\n",
      "Sri Chaitanya Techno School April 2020 – Jun 2022\n",
      "HSC-CBSE – Score: 89.2% Bengaluru\n",
      "Shemford Futuristic School Jun 2019 – Mar 2020\n",
      "SSLC-CBSE – Score: 85% Hosur\n",
      "Technical Skills\n",
      "Languages :Java,Python,C,SQL\n",
      "Technologies : Machine Learning, Artiﬁcial Intelligence, Deep Learning, Cloud Computing, NLP, Generative AI\n",
      "Concepts : Database Management System, Object Oriented Programming, Data structures and Algorithms\n",
      "Projects\n",
      "Real-Time Pronunciation and Language Learning assistance |Deep Learning, Python Aug 2024 - Dec 2024\n",
      "- Developed a system for users with diﬃculty learning and pronouncing.\n",
      "- The moto of the project was to help users learn the correct pronunciations through phonemes through\n",
      "corrected phonemes for their pronunciation.\n",
      "Dynamic Resource Allocation using Machine Learning Algorithms in AWS |Machine Learning,AWS,Cloud\n",
      "Computing, Python July 2024 - Dec 2024\n",
      "- The work involves allocating resources dynamically to online platforms based on their uses and optimising the\n",
      "resource usage\n",
      "- It involved using a ML algorithm for resource prediction and Ant Colony Algorithm for resource optimization.\n",
      "Chess Engine |Artiﬁcial Intelligence, Python Aug 2023 - Dec 2023\n",
      "- Developed a real-time playing chess bot using a modern AI heuristic algorithm.\n",
      "- It was developed to showcase a chessbot without the involvement of Machine Learning.\n",
      "Publications and Achievements\n",
      "•Published a paper titled ”Real-Time Pronunciation and Language Learning Assistance using Deep Learning” in\n",
      "ICECCC-25\n",
      "•Published a paper titled ”Blockchain-Based Secure Voting System: A Transparent and Tamper-Proof Approach to\n",
      "Modern Elections” in ICRDICCT-25\n",
      "•Published a paper titled ”Eﬃciency through Algorithms, chess without Machine Learning” in ICIAET-24\n",
      "•Qualiﬁed to the 2nd round of SIH-23\n",
      "•Solved over 300+ problems on Leetcode .(LeetCode Proﬁle)\n",
      "Certiﬁcations\n",
      "•Deep Learning and Reinforcement Learning-IBM\n",
      "•Machine Learning with Python-IBM\n",
      "•Generative AI for Everyone-DeepLearning.AI\n"
     ]
    }
   ],
   "source": [
    "# extrat text from each page separately\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V HARISH\\n♂phone9047214841 /envel⌢peharishvreddy10@gmail.com /linkedinLinkedIn /githubGithub\\nCareer Objective\\nMotivated Tech-Enthusiast skilled in programming with a passion for problem-solving. Quick learner with an\\never-growing hunger for new technologies. Seeking a challenging position in a reputable organization to expand\\nand utilize my learning, skills, and knowledge.\\nAcademic Details\\nAmrita Vishwa Vidyapeetham Oct 2022 – Present\\nBachelor of Technology in Computer Science and Engineering, Artiﬁcial Intelligence – CGPA: 8.41 Bengaluru\\nSri Chaitanya Techno School April 2020 – Jun 2022\\nHSC-CBSE – Score: 89.2% Bengaluru\\nShemford Futuristic School Jun 2019 – Mar 2020',\n",
       " 'Sri Chaitanya Techno School April 2020 – Jun 2022\\nHSC-CBSE – Score: 89.2% Bengaluru\\nShemford Futuristic School Jun 2019 – Mar 2020\\nSSLC-CBSE – Score: 85% Hosur\\nTechnical Skills\\nLanguages :Java,Python,C,SQL\\nTechnologies : Machine Learning, Artiﬁcial Intelligence, Deep Learning, Cloud Computing, NLP, Generative AI\\nConcepts : Database Management System, Object Oriented Programming, Data structures and Algorithms\\nProjects\\nReal-Time Pronunciation and Language Learning assistance |Deep Learning, Python Aug 2024 - Dec 2024\\n- Developed a system for users with diﬃculty learning and pronouncing.\\n- The moto of the project was to help users learn the correct pronunciations through phonemes through',\n",
       " '- Developed a system for users with diﬃculty learning and pronouncing.\\n- The moto of the project was to help users learn the correct pronunciations through phonemes through\\ncorrected phonemes for their pronunciation.\\nDynamic Resource Allocation using Machine Learning Algorithms in AWS |Machine Learning,AWS,Cloud\\nComputing, Python July 2024 - Dec 2024\\n- The work involves allocating resources dynamically to online platforms based on their uses and optimising the\\nresource usage\\n- It involved using a ML algorithm for resource prediction and Ant Colony Algorithm for resource optimization.\\nChess Engine |Artiﬁcial Intelligence, Python Aug 2023 - Dec 2023',\n",
       " 'resource usage\\n- It involved using a ML algorithm for resource prediction and Ant Colony Algorithm for resource optimization.\\nChess Engine |Artiﬁcial Intelligence, Python Aug 2023 - Dec 2023\\n- Developed a real-time playing chess bot using a modern AI heuristic algorithm.\\n- It was developed to showcase a chessbot without the involvement of Machine Learning.\\nPublications and Achievements\\n•Published a paper titled ”Real-Time Pronunciation and Language Learning Assistance using Deep Learning” in\\nICECCC-25\\n•Published a paper titled ”Blockchain-Based Secure Voting System: A Transparent and Tamper-Proof Approach to\\nModern Elections” in ICRDICCT-25',\n",
       " 'ICECCC-25\\n•Published a paper titled ”Blockchain-Based Secure Voting System: A Transparent and Tamper-Proof Approach to\\nModern Elections” in ICRDICCT-25\\n•Published a paper titled ”Eﬃciency through Algorithms, chess without Machine Learning” in ICIAET-24\\n•Qualiﬁed to the 2nd round of SIH-23\\n•Solved over 300+ problems on Leetcode .(LeetCode Proﬁle)\\nCertiﬁcations\\n•Deep Learning and Reinforcement Learning-IBM\\n•Machine Learning with Python-IBM\\n•Generative AI for Everyone-DeepLearning.AI']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the long text into small chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700,\n",
    "                                               chunk_overlap=200,\n",
    "                                               length_function=len)\n",
    "\n",
    "chunks = text_splitter.split_text(text=text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V HARISH\\n♂phone9047214841 /envel⌢peharishvreddy10@gmail.com /linkedinLinkedIn /githubGithub\\nCareer Objective\\nMotivated Tech-Enthusiast skilled in programming with a passion for problem-solving. Quick learner with an\\never-growing hunger for new technologies. Seeking a challenging position in a reputable organization to expand\\nand utilize my learning, skills, and knowledge.\\nAcademic Details\\nAmrita Vishwa Vidyapeetham Oct 2022 – Present\\nBachelor of Technology in Computer Science and Engineering, Artiﬁcial Intelligence – CGPA: 8.41 Bengaluru\\nSri Chaitanya Techno School April 2020 – Jun 2022\\nHSC-CBSE – Score: 89.2% Bengaluru\\nShemford Futuristic School Jun 2019 – Mar 2020'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sri Chaitanya Techno School April 2020 – Jun 2022\\nHSC-CBSE – Score: 89.2% Bengaluru\\nShemford Futuristic School Jun 2019 – Mar 2020\\nSSLC-CBSE – Score: 85% Hosur\\nTechnical Skills\\nLanguages :Java,Python,C,SQL\\nTechnologies : Machine Learning, Artiﬁcial Intelligence, Deep Learning, Cloud Computing, NLP, Generative AI\\nConcepts : Database Management System, Object Oriented Programming, Data structures and Algorithms\\nProjects\\nReal-Time Pronunciation and Language Learning assistance |Deep Learning, Python Aug 2024 - Dec 2024\\n- Developed a system for users with diﬃculty learning and pronouncing.\\n- The moto of the project was to help users learn the correct pronunciations through phonemes through'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"linkedin.com/in/gopiashokan \\ngithub.com/gopiashokan \\nWORK EXPERIENCE \\nSenior Process Executive - Operations \\nMahendra Next Wealth IT India Pvt Ltd \\n05/2019 - 12/2022\\n, \\n \\nNamakkal\"\n",
    "\n",
    "The above text is common(overlap) for both chunks[0] and chunks[1].\n",
    "(chunk_overlap=200 - maximum length, it means length is not exceed 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai langchain faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = input('Enter you OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_with_openai(openai_api_key, chunks, analyze):\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    vectorstores = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "    docs = vectorstores.similarity_search(query=analyze, k=3)\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "\n",
    "    chain = load_qa_chain(llm=llm, chain_type='stuff')\n",
    "    response = chain.run(input_documents=docs, question=analyze)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query\n\u001b[0;32m     10\u001b[0m summary \u001b[38;5;241m=\u001b[39m resume_summary(query_with_chunks\u001b[38;5;241m=\u001b[39mchunks)\n\u001b[1;32m---> 11\u001b[0m summary_result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_resume_with_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalyze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_result)\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mprocess_resume_with_openai\u001b[1;34m(openai_api_key, chunks, analyze)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_qa_chain\n\u001b[0;32m      7\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[1;32m----> 8\u001b[0m vectorstores \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m docs \u001b[38;5;241m=\u001b[39m vectorstores\u001b[38;5;241m.\u001b[39msimilarity_search(query\u001b[38;5;241m=\u001b[39manalyze, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     12\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\vectorstores\\faiss.py:602\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    604\u001b[0m         texts,\n\u001b[0;32m    605\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    610\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:490\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:374\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    371\u001b[0m     _iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), _chunk_size)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 374\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m     batched_embeddings\u001b[38;5;241m.\u001b[39mextend(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    381\u001b[0m results: List[List[List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:100\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_with_retry\u001b[39m(embeddings: OpenAIEmbeddings, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     retry_decorator \u001b[38;5;241m=\u001b[39m \u001b[43m_create_retry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    104\u001b[0m         response \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:47\u001b[0m, in \u001b[0;36m_create_retry_decorator\u001b[1;34m(embeddings)\u001b[0m\n\u001b[0;32m     39\u001b[0m max_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Wait 2^x * 1 second between each retry starting with\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 4 seconds, then up to 10 seconds, then 10 seconds afterwards\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry(\n\u001b[0;32m     43\u001b[0m     reraise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(embeddings\u001b[38;5;241m.\u001b[39mmax_retries),\n\u001b[0;32m     45\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait_exponential(multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mmin_seconds, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mmax_seconds),\n\u001b[0;32m     46\u001b[0m     retry\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m---> 47\u001b[0m         retry_if_exception_type(\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241m.\u001b[39mTimeout)\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIError)\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError)\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError)\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError)\n\u001b[0;32m     52\u001b[0m     ),\n\u001b[0;32m     53\u001b[0m     before_sleep\u001b[38;5;241m=\u001b[39mbefore_sleep_log(logger, logging\u001b[38;5;241m.\u001b[39mWARNING),\n\u001b[0;32m     54\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "def resume_summary(query_with_chunks):\n",
    "    query = f''' need to detailed summarization of below resume and finally conclude them\n",
    "\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                {query_with_chunks}\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                '''\n",
    "    return query\n",
    "\n",
    "summary = resume_summary(query_with_chunks=chunks)\n",
    "summary_result = process_resume_with_openai(openai_api_key=openai_api_key, chunks=chunks, analyze=summary)\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengths of Gopinath Asokan's resume:\n",
      "\n",
      "1. Strong foundation in diverse industries: Gopinath's resume highlights his experience and expertise in various industries, showcasing his adaptability and ability to work in different environments.\n",
      "\n",
      "2. 5+ years of industry experience: Gopinath's extensive experience in the industry demonstrates his ability to handle real-world challenges and shows his level of expertise in the field.\n",
      "\n",
      "3. Strong problem-solving and project management skills: Gopinath's resume emphasizes his skills in problem-solving and project management, which are crucial in the field of data science. This indicates his ability to effectively handle complex problems and successfully manage projects.\n",
      "\n",
      "4. Analytical skills combined with artistic expertise: Gopinath's resume mentions his eagerness to merge his analytical skills with artistic expertise, indicating his ability to think creatively and produce impactful insights and innovations.\n",
      "\n",
      "5. Data-driven strategies: Gopinath's excitement to apply data-driven strategies to challenges shows his understanding of the importance of data analysis in decision-making and problem-solving.\n",
      "\n",
      "6. Proficient in technical skills: Gopinath possesses a wide range of technical skills, including Python, machine learning, NLP, Selenium, and more. This showcases his ability to utilize various tools and technologies to solve complex problems and deliver high-quality work.\n",
      "\n",
      "7. Relevant certifications: Gopinath's completion of the Microsoft AI-900 Azure AI Fundamentals certificate demonstrates his commitment to continuous learning and staying up-to-date with the latest technologies and advancements in the field of data science.\n",
      "\n",
      "8. Experienced in various projects: Gopinath's experience in projects such as AI Resume Analyzer and LinkedIn Scraper, Retail Sales Forecast using ML, and Industrial Copper Modeling using ML showcases his practical application of data science techniques and his ability to deliver successful projects.\n",
      "\n",
      "In conclusion, Gopinath Asokan's resume exhibits strengths in his industry experience, problem-solving skills, technical expertise, and project management abilities. His diverse background, eagerness to merge analytical and artistic skills, and relevant certifications make him a strong candidate in the field of data science.\n"
     ]
    }
   ],
   "source": [
    "def resume_strength(query_with_chunks):\n",
    "    query = f'''need to detailed analysis and explain of the strength of below resume and finally conclude them\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                {query_with_chunks}\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                '''\n",
    "    return query\n",
    "\n",
    "strength = resume_strength(query_with_chunks=summary_result)\n",
    "strength_result = openai(openai_api_key=openai_api_key, chunks=chunks, analyze=strength)\n",
    "print(strength_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided resume details, Gopinath Asokan has a strong foundation in diverse industries and is highly skilled in problem-solving and project management. However, there are a few weaknesses in the resume that can be improved:\n",
      "\n",
      "1. Lack of a clear career objective: The resume does not mention a specific career objective or goal. It would be beneficial to include a clear and concise objective statement that highlights Gopinath's career aspirations and how his skills and experience align with those goals.\n",
      "\n",
      "2. Incomplete work experience details: While the resume mentions Gopinath's job titles and responsibilities, it does not provide specific accomplishments or achievements in each role. Adding quantifiable achievements or results-oriented statements would strengthen the resume and demonstrate Gopinath's impact in previous positions.\n",
      "\n",
      "3. Limited information on education: The resume briefly mentions Gopinath's educational background, but it lacks details on specific coursework or projects related to data science. Including relevant coursework, research projects, or any notable academic achievements would enhance the resume's credibility and showcase Gopinath's academic abilities.\n",
      "\n",
      "4. Lack of focus on key technical skills: Although the resume mentions a wide range of technical skills, it does not highlight which skills are most relevant to the data science field. It would be helpful to prioritize and emphasize the key technical skills that directly align with the desired job roles in data science.\n",
      "\n",
      "To improve the resume, consider the following suggestions:\n",
      "\n",
      "1. Start with a strong career objective statement that clearly communicates Gopinath's goals and how his skills and experience align with those goals.\n",
      "\n",
      "2. Include specific accomplishments and achievements in each work experience entry, highlighting the impact Gopinath made in previous roles. Use quantitative metrics whenever possible to showcase results.\n",
      "\n",
      "3. Provide more details on relevant coursework, research projects, or academic achievements related to data science during Gopinath's Master's degree program.\n",
      "\n",
      "4. Prioritize and highlight the key technical skills that directly align with data science roles. Consider creating a separate section dedicated to technical skills, showcasing proficiency and experience in those areas.\n",
      "\n",
      "5. Consider including any relevant certifications, online courses, or workshops related to data science or machine learning.\n",
      "\n",
      "By addressing these weaknesses and implementing these improvements, Gopinath Asokan can create a stronger and more impactful resume that highlights his skills, experience, and potential in the field of data science.\n"
     ]
    }
   ],
   "source": [
    "def resume_weakness(query_with_chunks):\n",
    "    query = f'''need to detailed analysis and explain of the weakness of below resume and how to improve make a better resume.\n",
    "\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                {query_with_chunks}\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                '''\n",
    "    return query\n",
    "\n",
    "weakness = resume_weakness(query_with_chunks=summary_result)\n",
    "result_weakness = openai(openai_api_key=openai_api_key, chunks=chunks, analyze=weakness)\n",
    "print(result_weakness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, some potential job roles that Gopinath Asokan could apply to on LinkedIn include:\n",
      "\n",
      "1. Data Scientist\n",
      "2. Data Analyst\n",
      "3. Machine Learning Engineer\n",
      "4. Business Analyst\n",
      "5. Project Manager\n",
      "6. Operations Analyst\n",
      "7. Quality Assurance Engineer\n",
      "8. Sales Analyst\n",
      "9. AI Engineer\n",
      "10. Retail Analyst\n",
      "\n",
      "These job roles align with Gopinath's skills and experience in data science, problem-solving, project management, resume analysis, retail sales forecasting, and quality assurance.\n"
     ]
    }
   ],
   "source": [
    "def job_title_suggestion(query_with_chunks):\n",
    "\n",
    "    query = f''' what are the job roles i apply to likedin based on below?\n",
    "                  \n",
    "                  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                  {query_with_chunks}\n",
    "                  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                '''\n",
    "    return query\n",
    "\n",
    "suggestion = job_title_suggestion(query_with_chunks=summary_result)\n",
    "result_suggestion = openai(openai_api_key=openai_api_key, chunks=chunks, analyze=suggestion)\n",
    "print(result_suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
